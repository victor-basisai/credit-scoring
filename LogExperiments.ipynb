{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python script for training a model version\n",
    "\"\"\"\n",
    "# Core Packages\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "# Third Party\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn import metrics\n",
    "import utils.credit as utils\n",
    "\n",
    "# Bedrock\n",
    "import bdrk\n",
    "from bedrock_client.bedrock.analyzer.model_analyzer import ModelAnalyzer\n",
    "from bedrock_client.bedrock.analyzer import ModelTypes\n",
    "from bedrock_client.bedrock.api import BedrockApi\n",
    "from bedrock_client.bedrock.metrics.service import ModelMonitoringService\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environmental params for Bedrock HCL\n",
    "# These are usually captured in the Bedrock HCL\n",
    "DATA_DIR_LOCAL = \"data/creditdata\"\n",
    "SEED = 2\n",
    "TH = 0.5\n",
    "LR_REGULARIZER = 1e-1\n",
    "RF_N_ESTIMATORS = 100\n",
    "CB_ITERATIONS = 100\n",
    "\n",
    "ENV_PARAMS = {\n",
    "    \"DATA_DIR_LOCAL\": DATA_DIR_LOCAL, \n",
    "    \"SEED\": SEED, \n",
    "    \"TH\": TH,\n",
    "    \"LR_REGULARIZER\": LR_REGULARIZER,\n",
    "    \"RF_N_ESTIMATORS\": RF_N_ESTIMATORS,\n",
    "    \"CB_ITERATIONS\": CB_ITERATIONS\n",
    "}\n",
    "\n",
    "OUTPUT_MODEL_PATH = \"/artefact/model.pkl\"\n",
    "FEATURE_COLS_PATH = \"/artefact/feature_cols.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Params for Bedrock Client\n",
    "# Not required if training pipeline is run via UI\n",
    "PROJECT_ID = \"victor-sandbox\"\n",
    "PIPELINE_ID = \"rest-credit-scoring\"\n",
    "ENVIRONMENT_ID = \"sandbox-aws-production\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsampling...\n",
      "scaling...\n",
      "fitting...\n",
      "C: 0.1\n",
      "chaining pipeline...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# Extraneous columns (as might be determined through feature selection)\n",
    "drop_cols = ['ID']\n",
    "\n",
    "# --- Data ETL ---\n",
    "# Load into pandas dataframes\n",
    "# x_<name> : features\n",
    "# y_<name> : labels\n",
    "x_train, y_train = utils.load_dataset(os.path.join(DATA_DIR_LOCAL, 'creditdata_train_v2.csv'), drop_columns=drop_cols)\n",
    "x_test, y_test = utils.load_dataset(os.path.join(DATA_DIR_LOCAL, 'creditdata_test_v2.csv'), drop_columns=drop_cols)\n",
    "\n",
    "\n",
    "# --- Candidate Binary Classification Algos ---\n",
    "# MODEL 1: LOGISTIC REGRESSION\n",
    "# Use best parameters from a model selection and threshold tuning process\n",
    "model = utils.train_log_reg_model(x_train, y_train, seed=SEED, C=LR_REGULARIZER, upsample=True, verbose=True)\n",
    "model_name = \"logreg_model\"\n",
    "model_type = ModelTypes.LINEAR\n",
    "\n",
    "# MODEL 2: RANDOM FOREST\n",
    "# Uses default threshold of 0.5 and model parameters\n",
    "# model = utils.train_rf_model(x_train, y_train, seed=SEED, upsample=True, verbose=True)\n",
    "# model_name = \"randomforest_model\"\n",
    "# model_type = ModelTypes.TREE\n",
    "\n",
    "# MODEL 3: CATBOOST\n",
    "# Uses default threshold of 0.5 and model parameters\n",
    "# model = utils.train_catboost_model(x_train, y_train, seed=SEED, upsample=True, verbose=True)\n",
    "# model_name = \"catboost_model\"\n",
    "# model_type = ModelTypes.TREE\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Logging Metrics via Bedrock Client Library\n",
    "- This bypasses the need to write Bedrock HCL or run Git-linked training pipelines\n",
    "- Metrics will still appear as a \"pipeline\" in the \"Training\" tab in the workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can customize the logging logic here\n",
    "_logger = logging.getLogger(bdrk.utils.vars.Constants.MAIN_LOG)\n",
    "_logger.setLevel(logging.INFO)\n",
    "if not _logger.handlers:\n",
    "    _logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "\n",
    "# Visit https://bedrock.basis-ai.com/setting/token/ to get the personal access token.\n",
    "# os.environ[\"BEDROCK_API_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_notebook_metrics(model, x_train, \n",
    "                                x_test, y_test, \n",
    "                                best_th=0.5,\n",
    "                                model_name=\"tree_model\", \n",
    "                                model_type=ModelTypes.TREE,\n",
    "                                access_token=None,\n",
    "                                project_id=None,\n",
    "                                pipeline_id=None,\n",
    "                                environment_id=None,\n",
    "                                env_params=None):\n",
    "    \n",
    "    bdrk.init(access_token=access_token, project_id=project_id)\n",
    "    with bdrk.start_run(pipeline_id=pipeline_id, environment_id=environment_id):\n",
    "    \n",
    "        bdrk.log_params(env_params)\n",
    "        \n",
    "        \"\"\"Compute and log metrics.\"\"\"\n",
    "        test_prob = model.predict_proba(x_test)[:, 1]\n",
    "        test_pred = np.where(test_prob > best_th, 1, 0)\n",
    "\n",
    "        acc = metrics.accuracy_score(y_test, test_pred)\n",
    "        precision = metrics.precision_score(y_test, test_pred)\n",
    "        recall = metrics.recall_score(y_test, test_pred)\n",
    "        f1_score = metrics.f1_score(y_test, test_pred)\n",
    "        roc_auc = metrics.roc_auc_score(y_test, test_prob)\n",
    "        avg_prc = metrics.average_precision_score(y_test, test_prob)\n",
    "        print(\"Evaluation\\n\"\n",
    "              f\"  Accuracy          = {acc:.4f}\\n\"\n",
    "              f\"  Precision         = {precision:.4f}\\n\"\n",
    "              f\"  Recall            = {recall:.4f}\\n\"\n",
    "              f\"  F1 score          = {f1_score:.4f}\\n\"\n",
    "              f\"  ROC AUC           = {roc_auc:.4f}\\n\"\n",
    "              f\"  Average precision = {avg_prc:.4f}\")\n",
    "\n",
    "        # --- Bedrock-native Integrations ---\n",
    "        # Bedrock Logger: captures model metrics\n",
    "\n",
    "        # Optional: Log metrics for each training step\n",
    "        # evals = model.evals_result_[\"valid_0\"]\n",
    "        # for iteration in range(N_ESTIMATORS):\n",
    "        #    bdrk.log_metrics(metrics={\n",
    "        #        \"Training binary_logloss\": evals[\"binary_logloss\"][iteration],\n",
    "        #        \"Training roc_auc\": evals[\"roc_auc\"][iteration]\n",
    "        #    }, x=iteration)\n",
    "        \n",
    "        # Log into charts, the binary classifier from the predicted data\n",
    "        bdrk.log_binary_classifier_metrics(actual=y_test.astype(int).tolist(),\n",
    "                                           probability=test_prob.flatten().tolist())\n",
    "\n",
    "        # Log final key-value pairs\n",
    "        bdrk.log_metric(\"Accuracy\", acc)\n",
    "        bdrk.log_metric(\"Precision\", precision)\n",
    "        bdrk.log_metric(\"Recall\", recall)\n",
    "        bdrk.log_metric(\"F1 score\", f1_score)\n",
    "        bdrk.log_metric(\"ROC AUC\", roc_auc)\n",
    "        bdrk.log_metric(\"Avg precision\", avg_prc)\n",
    "\n",
    "        # Alternative: Log dict of key-value pairs\n",
    "        # e.g. bdrk.log_metrics(metrics={\"Accuracy\": acc})\n",
    "        \n",
    "        # Saving and log the model\n",
    "        with open(OUTPUT_MODEL_PATH, \"wb\") as model_file:\n",
    "            pickle.dump(model, model_file)\n",
    "        bdrk.log_model(OUTPUT_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BedrockClient initialized on project=victor-sandbox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:BedrockClient initialized on project=victor-sandbox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run started: rest-credit-scoring-run8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:Run started: rest-credit-scoring-run8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params updated: {'DATA_DIR_LOCAL': 'data/creditdata', 'SEED': '2', 'TH': '0.5', 'LR_REGULARIZER': '0.1', 'RF_N_ESTIMATORS': '100', 'CB_ITERATIONS': '100'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:Params updated: {'DATA_DIR_LOCAL': 'data/creditdata', 'SEED': '2', 'TH': '0.5', 'LR_REGULARIZER': '0.1', 'RF_N_ESTIMATORS': '100', 'CB_ITERATIONS': '100'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation\n",
      "  Accuracy          = 0.6679\n",
      "  Precision         = 0.8757\n",
      "  Recall            = 0.6684\n",
      "  F1 score          = 0.7581\n",
      "  ROC AUC           = 0.7260\n",
      "  Average precision = 0.8792\n",
      "Confusion matrix logged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:Confusion matrix logged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New metrics logged: with x=0, metrics={'Accuracy': 0.6678666666666667}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:New metrics logged: with x=0, metrics={'Accuracy': 0.6678666666666667}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New metrics logged: with x=0, metrics={'Precision': 0.8757290264692688}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:New metrics logged: with x=0, metrics={'Precision': 0.8757290264692688}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New metrics logged: with x=0, metrics={'Recall': 0.6683787022770074}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:New metrics logged: with x=0, metrics={'Recall': 0.6683787022770074}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New metrics logged: with x=0, metrics={'F1 score': 0.7581318574618896}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:New metrics logged: with x=0, metrics={'F1 score': 0.7581318574618896}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New metrics logged: with x=0, metrics={'ROC AUC': 0.7259526332686599}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:New metrics logged: with x=0, metrics={'ROC AUC': 0.7259526332686599}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New metrics logged: with x=0, metrics={'Avg precision': 0.8792124886768665}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:New metrics logged: with x=0, metrics={'Avg precision': 0.8792124886768665}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logged: /artefact/model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:Model logged: /artefact/model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model version created: rest-credit-scoring-v4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:Model version created: rest-credit-scoring-v4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Succeeded: rest-credit-scoring-run8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:Run Succeeded: rest-credit-scoring-run8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run exitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:Run exitted\n"
     ]
    }
   ],
   "source": [
    "# Log the Run\n",
    "compute_log_notebook_metrics(model=model, \n",
    "                             x_train=x_train, \n",
    "                             x_test=x_test, \n",
    "                             y_test=y_test, \n",
    "                             best_th=TH,\n",
    "                             model_name=model_name, \n",
    "                             model_type=model_type,\n",
    "                             access_token=os.environ[\"BEDROCK_API_TOKEN\"],\n",
    "                             project_id=PROJECT_ID,\n",
    "                             pipeline_id=PIPELINE_ID,\n",
    "                             environment_id=ENVIRONMENT_ID,\n",
    "                             env_params=ENV_PARAMS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the project\n",
    "# bdrk.init(project_id=PROJECT_ID)\n",
    "\n",
    "# with bdrk.start_run(pipeline_id=PIPELINE_ID, environment_id=ENVIRONMENT_ID):\n",
    "#     # Log related parameters\n",
    "#     bdrk.log_params({\"LR\": LR, \"NUM_LEAVES\": NUM_LEAVES, \"N_ESTIMATORS\": N_ESTIMATORS})\n",
    "    \n",
    "#     # Log a single metric\n",
    "#     bdrk.log_metric(key=\"Feature Count\", value=len(FEATURE_COLS))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # -------------------------------------\n",
    "# # Bedrock functions - Training Pipeline\n",
    "# # -------------------------------------\n",
    "\n",
    "# def compute_log_metrics(model, x_train, \n",
    "#                         x_test, y_test, \n",
    "#                         best_th=0.5,\n",
    "#                         model_name=\"tree_model\", \n",
    "#                         model_type=ModelTypes.TREE):\n",
    "#     \"\"\"Compute and log metrics.\"\"\"\n",
    "#     test_prob = model.predict_proba(x_test)[:, 1]\n",
    "#     test_pred = np.where(test_prob > best_th, 1, 0)\n",
    "\n",
    "#     acc = metrics.accuracy_score(y_test, test_pred)\n",
    "#     precision = metrics.precision_score(y_test, test_pred)\n",
    "#     recall = metrics.recall_score(y_test, test_pred)\n",
    "#     f1_score = metrics.f1_score(y_test, test_pred)\n",
    "#     roc_auc = metrics.roc_auc_score(y_test, test_prob)\n",
    "#     avg_prc = metrics.average_precision_score(y_test, test_prob)\n",
    "#     print(\"Evaluation\\n\"\n",
    "#           f\"  Accuracy          = {acc:.4f}\\n\"\n",
    "#           f\"  Precision         = {precision:.4f}\\n\"\n",
    "#           f\"  Recall            = {recall:.4f}\\n\"\n",
    "#           f\"  F1 score          = {f1_score:.4f}\\n\"\n",
    "#           f\"  ROC AUC           = {roc_auc:.4f}\\n\"\n",
    "#           f\"  Average precision = {avg_prc:.4f}\")\n",
    "\n",
    "#     # --- Bedrock-native Integrations ---\n",
    "#     # Bedrock Logger: captures model metrics\n",
    "#     bedrock = BedrockApi(logging.getLogger(__name__))\n",
    "\n",
    "#     # Log into a chart\n",
    "#     bedrock.log_chart_data(y_test.astype(int).tolist(),\n",
    "#                            test_prob.flatten().tolist())\n",
    "\n",
    "#     # Log key-value pairs\n",
    "#     bedrock.log_metric(\"Accuracy\", acc)\n",
    "#     bedrock.log_metric(\"Precision\", precision)\n",
    "#     bedrock.log_metric(\"Recall\", recall)\n",
    "#     bedrock.log_metric(\"F1 score\", f1_score)\n",
    "#     bedrock.log_metric(\"ROC AUC\", roc_auc)\n",
    "#     bedrock.log_metric(\"Avg precision\", avg_prc)\n",
    "\n",
    "#     # Bedrock Model Analyzer: generates model explainability and fairness metrics\n",
    "#     # Analyzer (optional): generate explainability metrics\n",
    "#     analyzer = ModelAnalyzer(model[1], model_name=model_name, model_type=model_type)\\\n",
    "#                     .train_features(x_train)\\\n",
    "#                     .test_features(x_test)\n",
    "    \n",
    "#     # Analyzer (optional): generate fairness metrics\n",
    "#     analyzer.fairness_config(CONFIG_FAI)\\\n",
    "#         .test_labels(y_test)\\\n",
    "#         .test_inference(test_pred)\n",
    "    \n",
    "#     # Return the 4 metrics\n",
    "#     return analyzer.analyze()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
