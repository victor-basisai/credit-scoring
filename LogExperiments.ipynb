{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python script for training a model version\n",
    "\"\"\"\n",
    "# Core Packages\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Third Party\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, cross_validate, cross_val_score\n",
    "from sklearn.calibration import calibration_curve\n",
    "from sklearn import metrics\n",
    "import utils.credit as utils\n",
    "\n",
    "# Bedrock\n",
    "from bedrock_client.bedrock.analyzer.model_analyzer import ModelAnalyzer\n",
    "from bedrock_client.bedrock.analyzer import ModelTypes\n",
    "from bedrock_client.bedrock.api import BedrockApi\n",
    "from bedrock_client.bedrock.metrics.service import ModelMonitoringService\n",
    "import pickle\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environmental params for Bedrock HCL\n",
    "DATA_DIR_LOCAL = \"data/creditdata\"\n",
    "SEED = 0\n",
    "TH = 0.5\n",
    "LR_REGULARIZER = 1e-1\n",
    "RF_N_ESTIMATORS = 100\n",
    "CB_ITERATIONS = 100\n",
    "\n",
    "OUTPUT_MODEL_PATH = \"/artefact/model.pkl\"\n",
    "FEATURE_COLS_PATH = \"/artefact/feature_cols.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_PARAMS = {\n",
    "    \"DATA_DIR_LOCAL\": DATA_DIR_LOCAL, \n",
    "    \"SEED\": SEED, \n",
    "    \"TH\": TH,\n",
    "    \"LR_REGULARIZER\": LR_REGULARIZER,\n",
    "    \"RF_N_ESTIMATORS\": RF_N_ESTIMATORS,\n",
    "    \"CB_ITERATIONS\": CB_ITERATIONS\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------\n",
    "# Bedrock functions\n",
    "# ---------------------------------\n",
    "\n",
    "def compute_log_metrics(model, x_train, \n",
    "                        x_test, y_test, \n",
    "                        best_th=0.5,\n",
    "                        model_name=\"tree_model\", \n",
    "                        model_type=ModelTypes.TREE):\n",
    "    \"\"\"Compute and log metrics.\"\"\"\n",
    "    test_prob = model.predict_proba(x_test)[:, 1]\n",
    "    test_pred = np.where(test_prob > best_th, 1, 0)\n",
    "\n",
    "    acc = metrics.accuracy_score(y_test, test_pred)\n",
    "    precision = metrics.precision_score(y_test, test_pred)\n",
    "    recall = metrics.recall_score(y_test, test_pred)\n",
    "    f1_score = metrics.f1_score(y_test, test_pred)\n",
    "    roc_auc = metrics.roc_auc_score(y_test, test_prob)\n",
    "    avg_prc = metrics.average_precision_score(y_test, test_prob)\n",
    "    print(\"Evaluation\\n\"\n",
    "          f\"  Accuracy          = {acc:.4f}\\n\"\n",
    "          f\"  Precision         = {precision:.4f}\\n\"\n",
    "          f\"  Recall            = {recall:.4f}\\n\"\n",
    "          f\"  F1 score          = {f1_score:.4f}\\n\"\n",
    "          f\"  ROC AUC           = {roc_auc:.4f}\\n\"\n",
    "          f\"  Average precision = {avg_prc:.4f}\")\n",
    "\n",
    "    # --- Bedrock-native Integrations ---\n",
    "    # Bedrock Logger: captures model metrics\n",
    "    bedrock = BedrockApi(logging.getLogger(__name__))\n",
    "\n",
    "    # Log into a chart\n",
    "    bedrock.log_chart_data(y_test.astype(int).tolist(),\n",
    "                           test_prob.flatten().tolist())\n",
    "\n",
    "    # Log key-value pairs\n",
    "    bedrock.log_metric(\"Accuracy\", acc)\n",
    "    bedrock.log_metric(\"Precision\", precision)\n",
    "    bedrock.log_metric(\"Recall\", recall)\n",
    "    bedrock.log_metric(\"F1 score\", f1_score)\n",
    "    bedrock.log_metric(\"ROC AUC\", roc_auc)\n",
    "    bedrock.log_metric(\"Avg precision\", avg_prc)\n",
    "\n",
    "    # Bedrock Model Analyzer: generates model explainability and fairness metrics\n",
    "    # Analyzer (optional): generate explainability metrics\n",
    "    analyzer = ModelAnalyzer(model[1], model_name=model_name, model_type=model_type)\\\n",
    "                    .train_features(x_train)\\\n",
    "                    .test_features(x_test)\n",
    "    \n",
    "    # Analyzer (optional): generate fairness metrics\n",
    "    analyzer.fairness_config(CONFIG_FAI)\\\n",
    "        .test_labels(y_test)\\\n",
    "        .test_inference(test_pred)\n",
    "    \n",
    "    # Return the 4 metrics\n",
    "    return analyzer.analyze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upsampling...\n",
      "scaling...\n",
      "fitting...\n",
      "C: 0.1\n",
      "chaining pipeline...\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "# Extraneous columns (as might be determined through feature selection)\n",
    "drop_cols = ['ID']\n",
    "\n",
    "# --- Data ETL ---\n",
    "# Load into pandas dataframes\n",
    "# x_<name> : features\n",
    "# y_<name> : labels\n",
    "x_train, y_train = utils.load_dataset(os.path.join(DATA_DIR_LOCAL, 'creditdata_train_v2.csv'), drop_columns=drop_cols)\n",
    "x_test, y_test = utils.load_dataset(os.path.join(DATA_DIR_LOCAL, 'creditdata_test_v2.csv'), drop_columns=drop_cols)\n",
    "\n",
    "\n",
    "# --- Candidate Binary Classification Algos ---\n",
    "# MODEL 1: LOGISTIC REGRESSION\n",
    "# Use best parameters from a model selection and threshold tuning process\n",
    "model = utils.train_log_reg_model(x_train, y_train, seed=SEED, C=LR_REGULARIZER, upsample=True, verbose=True)\n",
    "model_name = \"logreg_model\"\n",
    "model_type = ModelTypes.LINEAR\n",
    "\n",
    "# MODEL 2: RANDOM FOREST\n",
    "# Uses default threshold of 0.5 and model parameters\n",
    "# model = utils.train_rf_model(x_train, y_train, seed=SEED, upsample=True, verbose=True)\n",
    "# model_name = \"randomforest_model\"\n",
    "# model_type = ModelTypes.TREE\n",
    "\n",
    "# MODEL 3: CATBOOST\n",
    "# Uses default threshold of 0.5 and model parameters\n",
    "# model = utils.train_catboost_model(x_train, y_train, seed=SEED, upsample=True, verbose=True)\n",
    "# model_name = \"catboost_model\"\n",
    "# model_type = ModelTypes.TREE\n",
    "\n",
    "\n",
    "# --- Bedrock-native Integrations ---\n",
    "# Bedrock Model Analyzer: generated values\n",
    "# (\n",
    "#     shap_values, \n",
    "#     base_shap_values, \n",
    "#     global_explainability, \n",
    "#     fairness_metrics,\n",
    "# ) = compute_log_metrics(model=model, x_train=x_train, \n",
    "#                         x_test=x_test, y_test=y_test, \n",
    "#                         best_th=TH,\n",
    "#                         model_name=model_name, model_type=model_type)\n",
    "\n",
    "# IMPORTANT: Saving the Model Artefact  Bedrock\n",
    "# with open(OUTPUT_MODEL_PATH, \"wb\") as model_file:\n",
    "#     pickle.dump(model, model_file)\n",
    "\n",
    "# Bedrock Model Monitoring: pre-requisite for monitoring concept drift\n",
    "# Prepare the inference probabilities\n",
    "# train_prob = model.predict_proba(x_train)[:, 1]\n",
    "# train_pred = np.where(train_prob > TH, 1, 0)\n",
    "\n",
    "# This step initialises the distribution from model training     \n",
    "# ModelMonitoringService.export_text(\n",
    "#     features=x_train.iteritems(),\n",
    "#     inference=train_prob.tolist(),\n",
    "# )\n",
    "# --- End of Bedrock-native Integrations ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logging an Experiment over REST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eyJ0eXAiOiJKV1QiLCJhbGciOiJSUzI1NiJ9.eyJ0b2tlbl9pZCI6ImNkNWMwOTgwLWE3M2UtNDA5Ni05ZTc0LTFlZTBiY2YzNTg5YiIsInVzZXJfaWQiOiI5NmU4Y2NmMS0zODFkLTQ2Y2ItYmU2MC1lYjQxODlmYmYxNGIiLCJpYXQiOjE2MjYyNDM4NDMsIm5iZiI6MTYyNjI0Mzg0M30.gb-60JgiNY8eXfYZ6iGQKNHVy3Cj07x9Db_LQriFaPLAW7AXfUFGLgVK2-j0jxM7dOb5ne_7WOEMpHpObiJstmZSMu_CZphPjI8vwmYYKlbemVE4N-_0es9zNolErvHBBcfRPbUzF_GFxt613KV560PoD-PEkh-niBStPOY2hvMdYpaBtbnEQArabBTS8M-pnZWYCBRovILifPryN8EylXycSiLKADcQYqEUQ7sOQxYS4pkqP-BBEZepoNF1GU69Z8MYkT2cOCuBEpMpZkRYzr0bgmAhje-BMZO8mFjm9vnUDb3XX-vP6hJai6cgCVjazUTUJuRISMS7pqdZjWvZwwAyvsMvOaQKf17mXiW9QCpQ7fOv9evAFUz5VmjiP9SqOVTzpEnMfggYXMxKrrvhaOBHQRCd4eLXHa8C2L6SuA4PEpg1l_PFCDjjkC7gor0MtonVWSfvWAQHboJ1nIIenW0rlfDp0YglVlqnFzW_qi0XBU-vdRE8_msp5bM5L1w_XB8ONMS_VXfde7i2zgqdt02ENXtZbr6BIrDPLqNSUX8gozCDEoQGkF1y3OTEp_qH_yaU9GKcj1i44uZ7y1C2Vg67izFiKEp1WWbw8rKp4yK7ktMAMHOA_NHNgckb6AmBx37BS1dU4um7GJBOh7vL9pUcGQPvcZRUpeCbhOtYGvM'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import bdrk\n",
    "import logging\n",
    "import sys\n",
    "import pickle\n",
    "from sklearn import metrics\n",
    "\n",
    "# You can customize the logging logic here\n",
    "_logger = logging.getLogger(bdrk.utils.vars.Constants.MAIN_LOG)\n",
    "_logger.setLevel(logging.INFO)\n",
    "if not _logger.handlers:\n",
    "    _logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "    \n",
    "# Visit https://bedrock.basis-ai.com/setting/token/ to get the personal access token.\n",
    "os.environ[\"BEDROCK_API_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"victor-sandbox\"\n",
    "PIPELINE_ID = \"rest-credit-scoring\"\n",
    "ENVIRONMENT_ID = \"sandbox-aws-production\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the project\n",
    "# bdrk.init(project_id=PROJECT_ID)\n",
    "\n",
    "# with bdrk.start_run(pipeline_id=PIPELINE_ID, environment_id=ENVIRONMENT_ID):\n",
    "#     # Log related parameters\n",
    "#     bdrk.log_params({\"LR\": LR, \"NUM_LEAVES\": NUM_LEAVES, \"N_ESTIMATORS\": N_ESTIMATORS})\n",
    "    \n",
    "#     # Log a single metric\n",
    "#     bdrk.log_metric(key=\"Feature Count\", value=len(FEATURE_COLS))\n",
    "    \n",
    "#     # Log metrics for each training step\n",
    "#     evals = clf.evals_result_[\"valid_0\"]\n",
    "#     for iteration in range(N_ESTIMATORS):\n",
    "#         bdrk.log_metrics(metrics={\n",
    "#             \"Training binary_logloss\": evals[\"binary_logloss\"][iteration],\n",
    "#             \"Training roc_auc\": evals[\"roc_auc\"][iteration]\n",
    "#         }, x=iteration)\n",
    "        \n",
    "    \n",
    "#     # Log final metrics in batch\n",
    "#     y_pred = (y_probs > 0.5).astype(int)\n",
    "#     bdrk.log_metrics(metrics={\n",
    "#         \"Accuracy\": metrics.accuracy_score(y_test, y_pred),\n",
    "#         \"Precision\": metrics.precision_score(y_test, y_pred),\n",
    "#         \"Recall\": metrics.recall_score(y_test, y_pred),\n",
    "#         \"F1 score\": metrics.f1_score(y_test, y_pred),\n",
    "#         \"Roc auc\": metrics.roc_auc_score(y_test, y_pred),\n",
    "#         \"Avg precision\": metrics.average_precision_score(y_test, y_pred),\n",
    "#     })\n",
    "    \n",
    "#     # Log the binary classifier from the predicted data\n",
    "#     bdrk.log_binary_classifier_metrics(actual=y_test, probability=y_probs)\n",
    "\n",
    "#     # Saving and log the model\n",
    "#     with open(OUTPUT_MODEL, \"wb\") as model_file:\n",
    "#         pickle.dump(clf, model_file)\n",
    "#     bdrk.log_model(OUTPUT_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_log_notebook_metrics(model, x_train, \n",
    "                                x_test, y_test, \n",
    "                                best_th=0.5,\n",
    "                                model_name=\"tree_model\", \n",
    "                                model_type=ModelTypes.TREE,\n",
    "                                project_id=None,\n",
    "                                pipeline_id=None,\n",
    "                                environment_id=None,\n",
    "                                env_params=None):\n",
    "    \n",
    "    bdrk.init(project_id=PROJECT_ID)\n",
    "    with bdrk.start_run(pipeline_id=PIPELINE_ID, environment_id=ENVIRONMENT_ID):\n",
    "    \n",
    "        bdrk.log_params(env_params)\n",
    "        \n",
    "        \"\"\"Compute and log metrics.\"\"\"\n",
    "        test_prob = model.predict_proba(x_test)[:, 1]\n",
    "        test_pred = np.where(test_prob > best_th, 1, 0)\n",
    "\n",
    "        acc = metrics.accuracy_score(y_test, test_pred)\n",
    "        precision = metrics.precision_score(y_test, test_pred)\n",
    "        recall = metrics.recall_score(y_test, test_pred)\n",
    "        f1_score = metrics.f1_score(y_test, test_pred)\n",
    "        roc_auc = metrics.roc_auc_score(y_test, test_prob)\n",
    "        avg_prc = metrics.average_precision_score(y_test, test_prob)\n",
    "        print(\"Evaluation\\n\"\n",
    "              f\"  Accuracy          = {acc:.4f}\\n\"\n",
    "              f\"  Precision         = {precision:.4f}\\n\"\n",
    "              f\"  Recall            = {recall:.4f}\\n\"\n",
    "              f\"  F1 score          = {f1_score:.4f}\\n\"\n",
    "              f\"  ROC AUC           = {roc_auc:.4f}\\n\"\n",
    "              f\"  Average precision = {avg_prc:.4f}\")\n",
    "\n",
    "        # --- Bedrock-native Integrations ---\n",
    "        # Bedrock Logger: captures model metrics\n",
    "\n",
    "        # Log the binary classifier from the predicted data\n",
    "#         bdrk.log_binary_classifier_metrics(actual=y_test, probability=y_pred)\n",
    "\n",
    "        # Log into a chart\n",
    "        bdrk.log_binary_classifier_metrics(y_test.astype(int).tolist(),\n",
    "                                           test_prob.flatten().tolist())\n",
    "\n",
    "        # Log key-value pairs\n",
    "        bdrk.log_metric(\"Accuracy\", acc)\n",
    "        bdrk.log_metric(\"Precision\", precision)\n",
    "        bdrk.log_metric(\"Recall\", recall)\n",
    "        bdrk.log_metric(\"F1 score\", f1_score)\n",
    "        bdrk.log_metric(\"ROC AUC\", roc_auc)\n",
    "        bdrk.log_metric(\"Avg precision\", avg_prc)\n",
    "\n",
    "        # Saving and log the model\n",
    "        with open(OUTPUT_MODEL_PATH, \"wb\") as model_file:\n",
    "            pickle.dump(model, model_file)\n",
    "        bdrk.log_model(OUTPUT_MODEL_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BedrockClient initialized on project=victor-sandbox\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:BedrockClient initialized on project=victor-sandbox\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run started: rest-credit-scoring-run7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:Run started: rest-credit-scoring-run7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params updated: {'DATA_DIR_LOCAL': 'data/creditdata', 'SEED': '0', 'TH': '0.5', 'LR_REGULARIZER': '0.1', 'RF_N_ESTIMATORS': '100', 'CB_ITERATIONS': '100'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:Params updated: {'DATA_DIR_LOCAL': 'data/creditdata', 'SEED': '0', 'TH': '0.5', 'LR_REGULARIZER': '0.1', 'RF_N_ESTIMATORS': '100', 'CB_ITERATIONS': '100'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation\n",
      "  Accuracy          = 0.6696\n",
      "  Precision         = 0.8758\n",
      "  Recall            = 0.6709\n",
      "  F1 score          = 0.7598\n",
      "  ROC AUC           = 0.7258\n",
      "  Average precision = 0.8787\n",
      "Confusion matrix logged\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:Confusion matrix logged\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New metrics logged: with x=0, metrics={'Accuracy': 0.6696}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:New metrics logged: with x=0, metrics={'Accuracy': 0.6696}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New metrics logged: with x=0, metrics={'Precision': 0.8757541899441341}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:New metrics logged: with x=0, metrics={'Precision': 0.8757541899441341}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New metrics logged: with x=0, metrics={'Recall': 0.6709467556925184}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:New metrics logged: with x=0, metrics={'Recall': 0.6709467556925184}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New metrics logged: with x=0, metrics={'F1 score': 0.7597906165180303}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:New metrics logged: with x=0, metrics={'F1 score': 0.7597906165180303}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New metrics logged: with x=0, metrics={'ROC AUC': 0.7257798817549945}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:New metrics logged: with x=0, metrics={'ROC AUC': 0.7257798817549945}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New metrics logged: with x=0, metrics={'Avg precision': 0.8787462755537497}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:New metrics logged: with x=0, metrics={'Avg precision': 0.8787462755537497}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model logged: /artefact/model.pkl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:Model logged: /artefact/model.pkl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model version created: rest-credit-scoring-v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:Model version created: rest-credit-scoring-v3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Succeeded: rest-credit-scoring-run7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:Run Succeeded: rest-credit-scoring-run7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run exitted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:bdrk:Run exitted\n"
     ]
    }
   ],
   "source": [
    "compute_log_notebook_metrics(model=model, \n",
    "                             x_train=x_train, \n",
    "                             x_test=x_test, \n",
    "                             y_test=y_test, \n",
    "                             best_th=TH,\n",
    "                             model_name=model_name, \n",
    "                             model_type=model_type,\n",
    "                             project_id=PROJECT_ID,\n",
    "                             pipeline_id=PIPELINE_ID,\n",
    "                             environment_id=ENVIRONMENT_ID,\n",
    "                             env_params=ENV_PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
